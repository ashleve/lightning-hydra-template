{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/icecube/moust/miniconda3/envs/icet2/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_hip.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Optional, Tuple, List, Iterator, Union\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Sampler, SequentialSampler #random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "import sqlite3\n",
    "import math\n",
    "# from torch import default_generator, randperm\n",
    "# from torch._utils import _accumulate\n",
    "# from torch.utils.data.dataset import Subset\n",
    "# from torch_geometric.data import Data\n",
    "# from torch.utils.data import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/upgrade_pure_numu_selection_event_no.csv\"\n",
    "db_path = \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/dev_step4_upgrade_028_with_noise_dynedge_pulsemap_v3_merger_aftercrash.db\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_csv_based_on_db_rows(db_path, csv_path, int_list, table_name, output_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Read the CSV file into a pandas DataFrame, considering it doesn't have a header\n",
    "    df = pd.read_csv(csv_path, ) #header=None\n",
    "\n",
    "    # The column name of the DataFrame is 0, let's rename it to 'event_number' for clarity\n",
    "    df.rename(columns={0: 'event_no'}, inplace=True)\n",
    "\n",
    "    # Count the number of rows for each event_number in the SQLite table\n",
    "    row_counts = df['event_no'].apply(lambda x: pd.read_sql_query(f\"SELECT COUNT(*) FROM {table_name} WHERE event_no = {x}\", conn).iloc[0,0])\n",
    "\n",
    "    # Attach the row counts to the original DataFrame\n",
    "    df['row_counts'] = row_counts\n",
    "\n",
    "    # The input list of integers is assumed to be sorted, if not, sort it\n",
    "    int_list.sort()\n",
    "\n",
    "    # Create the ranges for splitting the CSV file\n",
    "    ranges = [(0, int_list[0])] + [(int_list[i-1], int_list[i]) for i in range(1, len(int_list))] + [(int_list[-1], float('inf'))]\n",
    "\n",
    "    # For each range, select the rows with 'row_counts' within that range and write to a new CSV file\n",
    "    for i, (lower, upper) in enumerate(ranges):\n",
    "        df_range = df[(df['row_counts'] >= lower) & (df['row_counts'] < upper)]\n",
    "        df_range.drop(columns='row_counts').to_csv(f\"{output_path}/output_{i+1}.csv\", index=False)\n",
    "\n",
    "    # Close the SQLite connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = \"/groups/icecube/moust/storage/cached_event_no/upgrade_pure_numu_selection_event_no_test.csv\"\n",
    "train_csv = \"/groups/icecube/moust/storage/cached_event_no/upgrade_pure_numu_selection_event_no_train.csv\"\n",
    "val_csv = \"/groups/icecube/moust/storage/cached_event_no/upgrade_pure_numu_selection_event_no_val.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(val_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CascadeFilter_13</th>\n",
       "      <th>DeepCoreFilter_13</th>\n",
       "      <th>EventID</th>\n",
       "      <th>L3_oscNext_bool</th>\n",
       "      <th>L4_oscNext_bool</th>\n",
       "      <th>L5_oscNext_bool</th>\n",
       "      <th>L6_oscNext_bool</th>\n",
       "      <th>L7_oscNext_bool</th>\n",
       "      <th>MuonFilter_13</th>\n",
       "      <th>...</th>\n",
       "      <th>inelasticity</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>pid</th>\n",
       "      <th>position_x</th>\n",
       "      <th>position_y</th>\n",
       "      <th>position_z</th>\n",
       "      <th>sim_type</th>\n",
       "      <th>stopped_muon</th>\n",
       "      <th>track_length</th>\n",
       "      <th>zenith</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-6.710774</td>\n",
       "      <td>-62.118681</td>\n",
       "      <td>-189.266099</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.840785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>64.901104</td>\n",
       "      <td>-139.468445</td>\n",
       "      <td>-519.591668</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.916502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>93.307056</td>\n",
       "      <td>-127.478072</td>\n",
       "      <td>-418.871037</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.416366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062827</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>53.785919</td>\n",
       "      <td>-59.037806</td>\n",
       "      <td>-323.476400</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.501030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>96.607002</td>\n",
       "      <td>-179.181420</td>\n",
       "      <td>-486.508775</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.104446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102022</th>\n",
       "      <td>716640</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032871</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>200.529404</td>\n",
       "      <td>-2.128855</td>\n",
       "      <td>-343.039218</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.433850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102023</th>\n",
       "      <td>716644</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3076.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>139.997070</td>\n",
       "      <td>-138.179009</td>\n",
       "      <td>-333.642152</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.846469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102024</th>\n",
       "      <td>716647</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426457</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-66.024897</td>\n",
       "      <td>-61.042635</td>\n",
       "      <td>-242.314790</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.772950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102025</th>\n",
       "      <td>716650</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3099.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>222.274117</td>\n",
       "      <td>-125.957383</td>\n",
       "      <td>-334.514415</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.969339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102026</th>\n",
       "      <td>716663</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3184.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-220.676641</td>\n",
       "      <td>55.327720</td>\n",
       "      <td>-293.731291</td>\n",
       "      <td>genie</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.602828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102027 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  CascadeFilter_13  DeepCoreFilter_13  EventID   \n",
       "0               10              -1.0                1.0     36.0  \\\n",
       "1               22              -1.0                0.0     30.0   \n",
       "2               27              -1.0                1.0     30.0   \n",
       "3               35              -1.0                1.0    115.0   \n",
       "4               45              -1.0                1.0     98.0   \n",
       "...            ...               ...                ...      ...   \n",
       "102022      716640              -1.0                0.0   3050.0   \n",
       "102023      716644              -1.0                1.0   3076.0   \n",
       "102024      716647              -1.0                1.0   3084.0   \n",
       "102025      716650              -1.0                1.0   3099.0   \n",
       "102026      716663              -1.0                1.0   3184.0   \n",
       "\n",
       "        L3_oscNext_bool  L4_oscNext_bool  L5_oscNext_bool  L6_oscNext_bool   \n",
       "0                  -1.0             -1.0             -1.0             -1.0  \\\n",
       "1                  -1.0             -1.0             -1.0             -1.0   \n",
       "2                  -1.0             -1.0             -1.0             -1.0   \n",
       "3                  -1.0             -1.0             -1.0             -1.0   \n",
       "4                  -1.0             -1.0             -1.0             -1.0   \n",
       "...                 ...              ...              ...              ...   \n",
       "102022             -1.0             -1.0             -1.0             -1.0   \n",
       "102023             -1.0             -1.0             -1.0             -1.0   \n",
       "102024             -1.0             -1.0             -1.0             -1.0   \n",
       "102025             -1.0             -1.0             -1.0             -1.0   \n",
       "102026             -1.0             -1.0             -1.0             -1.0   \n",
       "\n",
       "        L7_oscNext_bool  MuonFilter_13  ...  inelasticity  interaction_type   \n",
       "0                  -1.0           -1.0  ...      0.290624               1.0  \\\n",
       "1                  -1.0           -1.0  ...      0.319193               1.0   \n",
       "2                  -1.0           -1.0  ...      0.167333               1.0   \n",
       "3                  -1.0           -1.0  ...      0.062827               1.0   \n",
       "4                  -1.0           -1.0  ...      0.441630               1.0   \n",
       "...                 ...            ...  ...           ...               ...   \n",
       "102022             -1.0           -1.0  ...      0.032871               1.0   \n",
       "102023             -1.0           -1.0  ...      0.100296               1.0   \n",
       "102024             -1.0           -1.0  ...      0.426457               1.0   \n",
       "102025             -1.0           -1.0  ...      0.074860               1.0   \n",
       "102026             -1.0           -1.0  ...      0.454551               1.0   \n",
       "\n",
       "         pid  position_x  position_y  position_z  sim_type  stopped_muon   \n",
       "0       14.0   -6.710774  -62.118681 -189.266099     genie          -1.0  \\\n",
       "1      -14.0   64.901104 -139.468445 -519.591668     genie          -1.0   \n",
       "2      -14.0   93.307056 -127.478072 -418.871037     genie          -1.0   \n",
       "3       14.0   53.785919  -59.037806 -323.476400     genie          -1.0   \n",
       "4      -14.0   96.607002 -179.181420 -486.508775     genie          -1.0   \n",
       "...      ...         ...         ...         ...       ...           ...   \n",
       "102022 -14.0  200.529404   -2.128855 -343.039218     genie          -1.0   \n",
       "102023 -14.0  139.997070 -138.179009 -333.642152     genie          -1.0   \n",
       "102024 -14.0  -66.024897  -61.042635 -242.314790     genie          -1.0   \n",
       "102025 -14.0  222.274117 -125.957383 -334.514415     genie          -1.0   \n",
       "102026 -14.0 -220.676641   55.327720 -293.731291     genie          -1.0   \n",
       "\n",
       "        track_length    zenith  \n",
       "0               -1.0  1.840785  \n",
       "1               -1.0  0.916502  \n",
       "2               -1.0  2.416366  \n",
       "3               -1.0  1.501030  \n",
       "4               -1.0  1.104446  \n",
       "...              ...       ...  \n",
       "102022          -1.0  1.433850  \n",
       "102023          -1.0  0.846469  \n",
       "102024          -1.0  1.772950  \n",
       "102025          -1.0  1.969339  \n",
       "102026          -1.0  1.602828  \n",
       "\n",
       "[102027 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_csv_based_on_db_rows(\n",
    "#     db_path,\n",
    "#     train_csv, \n",
    "#     [5,10,20,40,100,200], \n",
    "#     \"SplitInIcePulses_dynedge_v2_Pulses\", \n",
    "#     \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/train\")\n",
    "# split_csv_based_on_db_rows(\n",
    "#     db_path,\n",
    "#     test_csv, \n",
    "#     [5,10,20,40,100,200], \n",
    "#     \"SplitInIcePulses_dynedge_v2_Pulses\", \n",
    "#     \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/test\")\n",
    "split_csv_based_on_db_rows(\n",
    "    db_path,\n",
    "    val_csv, \n",
    "    [5,10,20,40,100,200], \n",
    "    \"SplitInIcePulses_dynedge_v2_Pulses\", \n",
    "    \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    6\n",
       "0  20\n",
       "1  28\n",
       "2  32\n",
       "3  46\n",
       "4  54"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file,)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_csv_based_on_db_rows(db_path, csv_file, [20,100], \"SplitInIcePulses_dynedge_v2_Pulses\", \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/upgrade_pure_numu_selection_event_no_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/event_groups/group1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m group1, group2, group3, group4 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray_split(sorted_event_nos, \u001b[39m4\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39m# Write event_no's for each group to a separate CSV file\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m# os.makedirs(\"/mnt/data/event_groups\", exist_ok=True)\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m np\u001b[39m.\u001b[39;49msavetxt(\u001b[39m\"\u001b[39;49m\u001b[39m/mnt/data/event_groups/group1.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, group1, delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m, fmt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     64\u001b[0m np\u001b[39m.\u001b[39msavetxt(\u001b[39m\"\u001b[39m\u001b[39m/mnt/data/event_groups/group2.csv\u001b[39m\u001b[39m\"\u001b[39m, group2, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, fmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m np\u001b[39m.\u001b[39msavetxt(\u001b[39m\"\u001b[39m\u001b[39m/mnt/data/event_groups/group3.csv\u001b[39m\u001b[39m\"\u001b[39m, group3, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, fmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/icet2/lib/python3.8/site-packages/numpy/lib/npyio.py:1541\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     fname \u001b[39m=\u001b[39m os_fspath(fname)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _is_string_like(fname):\n\u001b[1;32m   1540\u001b[0m     \u001b[39m# datasource doesn't support creating a new file ...\u001b[39;00m\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mopen\u001b[39;49m(fname, \u001b[39m'\u001b[39;49m\u001b[39mwt\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mclose()\n\u001b[1;32m   1542\u001b[0m     fh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlib\u001b[39m.\u001b[39m_datasource\u001b[39m.\u001b[39mopen(fname, \u001b[39m'\u001b[39m\u001b[39mwt\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39mencoding)\n\u001b[1;32m   1543\u001b[0m     own_fh \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/event_groups/group1.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Function to generate exponential distribution\n",
    "def generate_exponential_distribution(n, scale=50, size=1):\n",
    "    return expon.rvs(scale=scale, size=size)\n",
    "\n",
    "# Create a SQLite database in a file\n",
    "db_filename = \"events.db\"\n",
    "conn = sqlite3.connect(db_filename)\n",
    "c = conn.cursor()\n",
    "\n",
    "# # Create tables\n",
    "# c.execute(\"CREATE TABLE truth (event_no INTEGER PRIMARY KEY, energy REAL, inelasticity REAL)\")\n",
    "# c.execute(\"CREATE TABLE pulsemap (event_no INTEGER, dom_x REAL, dom_y REAL, dom_z REAL, time REAL, charge REAL)\")\n",
    "\n",
    "# # Populate tables\n",
    "# np.random.seed(0)\n",
    "# num_events = 10000\n",
    "\n",
    "# # Generate event numbers\n",
    "# event_nos = np.arange(num_events)\n",
    "\n",
    "# # Generate energies and inelasticities\n",
    "# energies = np.random.uniform(size=num_events)\n",
    "# inelasticities = np.random.uniform(size=num_events)\n",
    "\n",
    "# # Populate truth table\n",
    "# for event_no, energy, inelasticity in zip(event_nos, energies, inelasticities):\n",
    "#     # Convert numpy int64 to native int\n",
    "#     event_no = int(event_no)\n",
    "#     c.execute(\"INSERT INTO truth VALUES (?, ?, ?)\", (event_no, energy, inelasticity))\n",
    "\n",
    "# # Generate pulsemap data\n",
    "# for event_no in event_nos:\n",
    "#     num_rows = int(generate_exponential_distribution(n=1, scale=30, size=1))\n",
    "#     for _ in range(num_rows):\n",
    "#         dom_x, dom_y, dom_z, time, charge = np.random.uniform(size=5)\n",
    "#         # Convert numpy int64 to native int\n",
    "#         event_no = int(event_no)\n",
    "#         c.execute(\"INSERT INTO pulsemap VALUES (?, ?, ?, ?, ?, ?)\", (event_no, dom_x, dom_y, dom_z, time, charge))\n",
    "\n",
    "# # Commit changes\n",
    "# conn.commit()\n",
    "\n",
    "# Create histogram of the number of hits in the events\n",
    "c.execute(\"SELECT event_no, COUNT(*) FROM pulsemap GROUP BY event_no\")\n",
    "data = c.fetchall()\n",
    "event_nos, counts = zip(*data)\n",
    "\n",
    "# Divide event_no's into four groups based on the number of rows with the same event_no\n",
    "counts = np.array(counts)\n",
    "event_nos = np.array(event_nos)\n",
    "indices = counts.argsort()\n",
    "sorted_event_nos = event_nos[indices]\n",
    "group1, group2, group3, group4 = np.array_split(sorted_event_nos, 4)\n",
    "\n",
    "# Write event_no's for each group to a separate CSV file\n",
    "# os.makedirs(\"/mnt/data/event_groups\", exist_ok=True)\n",
    "np.savetxt(\"/mnt/data/event_groups/group1.csv\", group1, delimiter=\",\", fmt=\"%d\")\n",
    "np.savetxt(\"/mnt/data/event_groups/group2.csv\", group2, delimiter=\",\", fmt=\"%d\")\n",
    "np.savetxt(\"/mnt/data/event_groups/group3.csv\", group3, delimiter=\",\", fmt=\"%d\")\n",
    "np.savetxt(\"/mnt/data/event_groups/group4.csv\", group4, delimiter=\",\", fmt=\"%d\")\n",
    "\n",
    "db_filename, \"/mnt/data/event_groups/group1.csv\", \"/mnt/data/event_groups/group2.csv\", \"/mnt/data/event_groups/group3.csv\", \"/mnt/data/event_groups/group4.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1098831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1098832"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/upgrade_pure_numu_selection_event_no.csv\"\n",
    "db_path = \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/dev_step4_upgrade_028_with_noise_dynedge_pulsemap_v3_merger_aftercrash.db\"\n",
    "df = pd.read_csv(csv_file)\n",
    "print(len(df))\n",
    "csv_filenames = [\n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_1.csv\",\n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_2.csv\", \n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_3.csv\"\n",
    "    ]\n",
    "event_numbers = []\n",
    "for csv_filename in csv_filenames:\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    event_numbers.extend(df['event_no'].tolist())\n",
    "len(event_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT event_no, COUNT(*) FROM SplitInIcePulses_dynedge_v2_Pulses GROUP BY event_no\")\n",
    "data = c.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[39m.\u001b[39;49mshape()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables within the SQL database is:\n",
      "                                       name\n",
      "0                                     truth\n",
      "1                         pisa_dependencies\n",
      "2                          SplitInIcePulses\n",
      "3               SplitInIcePulses_TruthFlags\n",
      "4                   SplitIceCubePulsesTWSRT\n",
      "5        SplitIceCubePulsesTWSRT_TruthFlags\n",
      "6         SplitInIcePulses_GraphSage_Pulses\n",
      "7    SplitInIcePulses_GraphSage_Predictions\n",
      "8   SplitInIcePulses_dynedge_v2_Predictions\n",
      "9        SplitInIcePulses_dynedge_v2_Pulses\n",
      "10    no_upgrade_strings_dynedge_v2_cleaned\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_all =\"SELECT * FROM truth\"\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    db_tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type = 'table'\", conn)\n",
    "    print('Tables within the SQL database is:')\n",
    "    print(db_tables)\n",
    "    print()\n",
    "    mini_db = {name:  pd.read_sql_query(f\"SELECT * FROM {name} LIMIT 3000\", conn) for name in db_tables.name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(24.1367), tensor([[ 1.9434e+02, -3.0920e+01, -2.1078e+02,  1.1114e+04,  1.1386e+00],\n",
      "        [ 9.0490e+01,  8.2350e+01, -1.6214e+02,  1.0005e+04,  4.8686e-01],\n",
      "        [ 9.0490e+01,  8.2350e+01, -1.9619e+02,  9.8812e+03,  3.0716e-01],\n",
      "        [ 9.0490e+01,  8.2350e+01, -2.1321e+02,  1.0058e+04,  8.5537e-01],\n",
      "        [ 9.0490e+01,  8.2350e+01, -2.3023e+02,  9.9392e+03,  1.3410e+00],\n",
      "        [ 9.0490e+01,  8.2350e+01, -2.3023e+02,  9.9442e+03,  1.1514e+00],\n",
      "        [ 9.0490e+01,  8.2350e+01, -2.3023e+02,  9.9967e+03,  2.4345e-01],\n",
      "        [ 9.0490e+01,  8.2350e+01, -2.4725e+02,  9.9892e+03,  8.4372e-01],\n",
      "        [ 1.3203e+02,  2.0298e+02, -1.7809e+02,  1.0410e+04,  5.9601e-01],\n",
      "        [ 4.1600e+01,  3.5490e+01, -2.4901e+02,  1.0430e+04,  4.9121e-01],\n",
      "        [ 1.0694e+02,  2.7090e+01, -1.8880e+02,  1.0353e+04,  7.8922e-01],\n",
      "        [ 1.1319e+02, -6.0470e+01, -3.7837e+02,  1.1606e+04,  6.4532e-01],\n",
      "        [ 2.6957e+01, -3.1191e+01, -3.1668e+02,  1.0851e+04,  1.0357e+00]]))\n",
      "1098832\n"
     ]
    }
   ],
   "source": [
    "class ChunkDataset(Dataset):\n",
    "    def __init__(self, db_filename, csv_filenames):\n",
    "        # self.conn = sqlite3.connect(db_filename)\n",
    "        # self.c = self.conn.cursor()\n",
    "        # self.c.execute(\"SELECT event_no FROM truth\")\n",
    "        # self.event_nos = [row[0] for row in self.c.fetchall()]\n",
    "        self.conn = sqlite3.connect(db_filename)\n",
    "        self.c = self.conn.cursor()\n",
    "        self.event_nos = []\n",
    "        for csv_filename in csv_filenames:\n",
    "            df = pd.read_csv(csv_filename)\n",
    "            self.event_nos.extend(df['event_no'].tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_nos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        event_no = self.event_nos[idx]\n",
    "        self.c.execute(\"SELECT energy FROM truth WHERE event_no = ?\", (event_no,))\n",
    "        energy = self.c.fetchone()[0]\n",
    "        self.c.execute(\"SELECT dom_x, dom_y, dom_z, dom_time, charge FROM SplitInIcePulses_dynedge_v2_Pulses WHERE event_no = ?\", (event_no,))\n",
    "        pulsemap_data = self.c.fetchall()\n",
    "        return torch.tensor(energy, dtype=torch.float32), torch.tensor(pulsemap_data, dtype=torch.float32)\n",
    "    \n",
    "class ChunkSampler(Sampler):\n",
    "    def __init__(self, csv_filenames, batch_sizes):\n",
    "        self.event_nos = []\n",
    "        for csv_filename, batch_size in zip(csv_filenames, batch_sizes):\n",
    "            # event_nos = np.loadtxt(csv_filename, delimiter=\",\", dtype=int).tolist()\n",
    "            event_nos = pd.read_csv(csv_filename)['event_no'].tolist()\n",
    "            self.event_nos.extend([event_nos[i:i + batch_size] for i in range(0, len(event_nos), batch_size)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.event_nos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_nos)\n",
    "    \n",
    "def collate_fn(data):\n",
    "    # Split the data into energies and pulsemap data\n",
    "    energies, pulsemap_data = zip(*data)\n",
    "    \n",
    "    # Pad the pulsemap data\n",
    "    pulsemap_data = pad_sequence(pulsemap_data, batch_first=True)\n",
    "    \n",
    "    return torch.stack(energies), pulsemap_data\n",
    "\n",
    "csv_filenames = [\n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_1.csv\",\n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_2.csv\", \n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_3.csv\"\n",
    "    ]\n",
    "\n",
    "# csv_file = \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/upgrade_pure_numu_selection_event_no.csv\"\n",
    "db_path = \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/dev_step4_upgrade_028_with_noise_dynedge_pulsemap_v3_merger_aftercrash.db\"\n",
    "\n",
    "# Test the EventDataset\n",
    "dataset = ChunkDataset(db_path, csv_filenames ) #db_filename\n",
    "batch_sizes = [100, 50, 10]  # Adjust these batch sizes according to your machine's memory limits\n",
    "sampler = ChunkSampler(csv_filenames, batch_sizes)\n",
    "dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "print(dataset[0])  # Print the first sample from the dataset\n",
    "print(len(dataset))  # Print the total number of samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_numbers = []\n",
    "for csv_filename in csv_filenames:\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    event_numbers.extend(df['event_no'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37930\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'EventSampler' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m sampler \u001b[39m=\u001b[39m EventSampler(csv_filenames, batch_sizes)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(sampler))  \u001b[39m# Print the total number of batches in the sampler\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mprint\u001b[39m(sampler[\u001b[39m0\u001b[39;49m])  \u001b[39m# Print the first batch from the sampler\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'EventSampler' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class EventSampler(Sampler):\n",
    "    def __init__(self, csv_filenames, batch_sizes):\n",
    "        self.event_nos = []\n",
    "        for csv_filename, batch_size in zip(csv_filenames, batch_sizes):\n",
    "            # event_nos = np.loadtxt(csv_filename, delimiter=\",\", dtype=int).tolist()\n",
    "            event_nos = pd.read_csv(csv_filename)['event_no'].tolist()\n",
    "            self.event_nos.extend([event_nos[i:i + batch_size] for i in range(0, len(event_nos), batch_size)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.event_nos)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_nos)\n",
    "    \n",
    "def collate_fn(data):\n",
    "    # Split the data into energies and pulsemap data\n",
    "    energies, pulsemap_data = zip(*data)\n",
    "    \n",
    "    # Pad the pulsemap data\n",
    "    pulsemap_data = pad_sequence(pulsemap_data, batch_first=True)\n",
    "    \n",
    "    return torch.stack(energies), pulsemap_data\n",
    "\n",
    "csv_filenames = [\n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_1.csv\",\n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_2.csv\", \n",
    "    \"/groups/icecube/moust/storage/cached_event_no/output_3.csv\"\n",
    "    ]\n",
    "\n",
    "batch_sizes = [100, 50, 10]  # Adjust these batch sizes according to your machine's memory limits\n",
    "sampler = EventSampler(csv_filenames, batch_sizes)\n",
    "print(len(sampler))  # Print the total number of batches in the sampler\n",
    "# print(sampler[0])  # Print the first batch from the sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([24.1367, 18.3989]),\n",
       " tensor([[[ 1.9434e+02, -3.0920e+01, -2.1078e+02,  1.1114e+04,  1.1386e+00],\n",
       "          [ 9.0490e+01,  8.2350e+01, -1.6214e+02,  1.0005e+04,  4.8686e-01],\n",
       "          [ 9.0490e+01,  8.2350e+01, -1.9619e+02,  9.8812e+03,  3.0716e-01],\n",
       "          [ 9.0490e+01,  8.2350e+01, -2.1321e+02,  1.0058e+04,  8.5537e-01],\n",
       "          [ 9.0490e+01,  8.2350e+01, -2.3023e+02,  9.9392e+03,  1.3410e+00],\n",
       "          [ 9.0490e+01,  8.2350e+01, -2.3023e+02,  9.9442e+03,  1.1514e+00],\n",
       "          [ 9.0490e+01,  8.2350e+01, -2.3023e+02,  9.9967e+03,  2.4345e-01],\n",
       "          [ 9.0490e+01,  8.2350e+01, -2.4725e+02,  9.9892e+03,  8.4372e-01],\n",
       "          [ 1.3203e+02,  2.0298e+02, -1.7809e+02,  1.0410e+04,  5.9601e-01],\n",
       "          [ 4.1600e+01,  3.5490e+01, -2.4901e+02,  1.0430e+04,  4.9121e-01],\n",
       "          [ 1.0694e+02,  2.7090e+01, -1.8880e+02,  1.0353e+04,  7.8922e-01],\n",
       "          [ 1.1319e+02, -6.0470e+01, -3.7837e+02,  1.1606e+04,  6.4532e-01],\n",
       "          [ 2.6957e+01, -3.1191e+01, -3.1668e+02,  1.0851e+04,  1.0357e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 1.2497e+02, -1.3125e+02, -3.9142e+02,  1.1093e+04,  8.7884e-01],\n",
       "          [ 1.9434e+02, -3.0920e+01, -2.1078e+02,  9.8750e+03,  3.2512e-01],\n",
       "          [ 1.9434e+02, -3.0920e+01, -2.2780e+02,  9.9845e+03,  3.9750e-01],\n",
       "          [ 1.9434e+02, -3.0920e+01, -2.4482e+02,  9.9065e+03,  9.5361e-01],\n",
       "          [ 1.9434e+02, -3.0920e+01, -2.4482e+02,  1.0066e+04,  1.6089e-01],\n",
       "          [ 3.1250e+01, -7.2930e+01, -3.2033e+02,  1.1162e+04,  5.4170e-01],\n",
       "          [ 7.2370e+01, -6.6600e+01, -3.2103e+02,  1.1063e+04,  3.8089e-01],\n",
       "          [ 1.0694e+02,  2.7090e+01, -2.3786e+02,  1.1305e+04,  2.0699e-01],\n",
       "          [ 1.0694e+02,  2.7090e+01, -3.2196e+02,  1.0575e+04,  9.1849e-01],\n",
       "          [ 1.1319e+02, -6.0470e+01, -3.7837e+02,  1.0662e+04,  8.7170e-01],\n",
       "          [ 8.9415e+01, -5.9051e+01, -2.8387e+02,  1.0356e+04,  1.0106e+00],\n",
       "          [ 8.9415e+01, -5.8947e+01, -3.0199e+02,  1.0596e+04,  9.3420e-01],\n",
       "          [ 8.9415e+01, -5.8947e+01, -3.9487e+02,  1.0885e+04,  4.9063e-01],\n",
       "          [ 4.7342e+01, -5.6887e+01, -3.1987e+02,  1.0803e+04,  1.3093e+00]]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    # Split the data into energies and pulsemap data\n",
    "    energies, pulsemap_data = zip(*data)\n",
    "    \n",
    "    # Pad the pulsemap data\n",
    "    pulsemap_data = pad_sequence(pulsemap_data, batch_first=True)\n",
    "    \n",
    "    return torch.stack(energies), pulsemap_data\n",
    "\n",
    "# Test the collate function with the first two samples from the dataset\n",
    "collate_fn([dataset[0], dataset[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [100, 50, 10]  # Adjust these batch sizes according to your machine's memory limits\n",
    "sampler = EventSampler(csv_filenames, batch_sizes)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=collate_fn, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([100, 19, 5])\n"
     ]
    }
   ],
   "source": [
    "for energies, pulsemap_data in dataloader:\n",
    "    print(energies.shape)\n",
    "    print(pulsemap_data.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch dataset for loading chunked data from an SQLite database.\n",
    "    This dataset retrieves pulsemap and truth data for each event from the database.\n",
    "\n",
    "    Args:\n",
    "        db_filename (str): Filename of the SQLite database.\n",
    "        csv_filenames (list of str): List of CSV filenames containing event numbers.\n",
    "        pulsemap_table (str): Name of the table containing pulsemap data.\n",
    "        truth_table (str): Name of the table containing truth data.\n",
    "        truth_variable (str): Name of the variable to query from the truth table.\n",
    "        feature_variables (list of str): List of variable names to query from the pulsemap table.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str,\n",
    "        csv_filenames: List[str],\n",
    "        pulsemap_table: str,\n",
    "        truth_table: str,\n",
    "        truth_variable: str,\n",
    "        feature_variables: List[str]\n",
    "    ) -> None:\n",
    "        self.conn = sqlite3.connect(db_path)  # Connect to the SQLite database\n",
    "        self.c = self.conn.cursor()\n",
    "        self.event_nos = []\n",
    "        for csv_filename in csv_filenames:\n",
    "            df = pd.read_csv(csv_filename)\n",
    "            self.event_nos.extend(df['event_no'].tolist())  # Collect event numbers from CSV files\n",
    "        self.pulsemap_table = pulsemap_table  # Name of the table containing pulsemap data\n",
    "        self.truth_table = truth_table  # Name of the table containing truth data\n",
    "        self.truth_variable = truth_variable  # Name of the variable to query from the truth table\n",
    "        self.feature_variables = feature_variables  # List of variable names to query from the pulsemap table\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.event_nos)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        event_no = self.event_nos[idx]\n",
    "        # Query the truth variable for the given event number\n",
    "        self.c.execute(f\"SELECT {self.truth_variable} FROM {self.truth_table} WHERE event_no = ?\", (event_no,))\n",
    "        truth_value = self.c.fetchone()[0]\n",
    "        feature_query = ', '.join(self.feature_variables)\n",
    "        # Query the feature variables from the pulsemap table for the given event number\n",
    "        self.c.execute(f\"SELECT {feature_query} FROM {self.pulsemap_table} WHERE event_no = ?\", (event_no,))\n",
    "        pulsemap_data = self.c.fetchall()\n",
    "        return torch.tensor(truth_value, dtype=torch.float32), torch.tensor(pulsemap_data, dtype=torch.float32)\n",
    "    \n",
    "    def close_connection(self) -> None:\n",
    "        self.conn.close()\n",
    "\n",
    "\n",
    "class ChunkSampler(Sampler):\n",
    "    \"\"\"\n",
    "    PyTorch sampler for creating chunks from event numbers.\n",
    "\n",
    "    Args:\n",
    "        csv_filenames (List[str]): List of CSV filenames containing event numbers.\n",
    "        batch_sizes (List[int]): List of batch sizes for each CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_filenames: List[str], batch_sizes: List[int]) -> None:\n",
    "        self.event_nos = []\n",
    "        for csv_filename, batch_size in zip(csv_filenames, batch_sizes):\n",
    "            event_nos = pd.read_csv(csv_filename)['event_no'].tolist()\n",
    "            self.event_nos.extend([event_nos[i:i + batch_size] for i in range(0, len(event_nos), batch_size)])\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return iter(self.event_nos)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.event_nos)\n",
    "\n",
    "\n",
    "def collate_fn(data: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    # Split the data into truths and pulsemap data\n",
    "    truths, pulsemap_data = zip(*data)\n",
    "\n",
    "    # Pad the pulsemap data\n",
    "    pulsemap_lengths = [len(x) for x in pulsemap_data]\n",
    "    pulsemap_data = pad_sequence(pulsemap_data, batch_first=True, padding_value=0)\n",
    "\n",
    "    pad_mask = torch.zeros_like(pulsemap_data[:, :, 0]).type(torch.bool)\n",
    "\n",
    "    for i, length in enumerate(pulsemap_lengths):\n",
    "        pad_mask[i, length:] = True\n",
    "\n",
    "    return (pulsemap_data, torch.stack(truths), pad_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_csv_train = [\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/train/output_1.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/train/output_2.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/train/output_3.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/train/output_4.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/train/output_5.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/train/output_6.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/train/output_7.csv\",\n",
    "]\n",
    "chunk_csv_test = [\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/test/output_1.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/test/output_2.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/test/output_3.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/test/output_4.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/test/output_5.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/test/output_6.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/test/output_7.csv\",\n",
    "]\n",
    "chunk_csv_val = [\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/val/output_1.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/val/output_2.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/val/output_3.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/val/output_4.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/val/output_5.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/val/output_6.csv\",\n",
    "  \"/groups/icecube/moust/storage/cached_event_no/upgrade_numu/val/output_7.csv\",\n",
    "]\n",
    "\n",
    "batch_sizes = [512, 256, 128, 64, 32, 16, 8]\n",
    "truth_table = \"truth\"\n",
    "db_path = \"/groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Data/sqlite3/dev_step4_upgrade_028_with_noise_dynedge_pulsemap_v3_merger_aftercrash.db\"\n",
    "pulsemap = \"SplitInIcePulses_dynedge_v2_Pulses\"\n",
    "input_cols =  [\"dom_x\", \"dom_y\", \"dom_z\", \"dom_time\", \"charge\"]\n",
    "target_cols = \"inelasticity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ChunkDataset(\n",
    "    db_path = db_path,\n",
    "    csv_filenames = chunk_csv_train,\n",
    "    pulsemap_table = pulsemap, \n",
    "    truth_table = truth_table, \n",
    "    feature_variables = input_cols, \n",
    "    truth_variable = target_cols, \n",
    "    )\n",
    "# data_test = ChunkDataset(db_path, pulsemap, truth_table, input_cols, target_cols, chunk_csv_test)\n",
    "# data_val = ChunkDataset(db_path, pulsemap, truth_table, input_cols, target_cols, chunk_csv_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset = data_train,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_sampler=ChunkSampler(chunk_csv_train, batch_sizes),\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 39, 5])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for i, (features, _, _) in enumerate(dl):\n",
    "    print(features.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7f3ae53592e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(dataloader_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icet2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
