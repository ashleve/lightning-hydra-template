## PyTorch Lightning + Hydra template 
### A clean and simple template to kickstart your deep learning project ðŸš€âš¡ðŸ”¥<br>
- structures ML code the same so that work can easily be extended and replicated
- allows for rapid experimentation by automating pipeline with config files
- extends functionality of popular experiment loggers like Weights&Biases (mostly with dedicated callbacks)

This template tries to be as generic as possible - you should be able to easily modify behavior in [train.py](project/train.py) in case you need some unconventional configuration wiring.<br>

Click on <b>`Use this template`</b> button above to initialize new repository.<br>




### Why Lightning + Hydra?
- <b>[PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning)</b> provides great abstractions for well structured ML code and advanced features like checkpointing, gradient accumulation, distributed training, etc.<br>
- <b>[Hydra](https://github.com/facebookresearch/hydra)</b> provides convenient way to manage experiment configurations and advanced features like overriding any config parameter from command line, scheduling execution of many runs, etc.<br>



### Some Notes
***\*warning: this template currently uses development version of hydra which might be unstable (we wait until version 1.1 is released)*** <br>
*\*based on [deep-learninig-project-template](https://github.com/PyTorchLightning/deep-learning-project-template) by PyTorchLightning organization.*<br>
*\*Suggestions are always welcome!*


## Features
- Predefined folder structure
- Modularity: all abstractions are splitted into different submodules
- Automates PyTorch Lightning training pipeline with little boilerplate, so it can be easily modified (see [train.py](project/train.py))
- All advantages of Hydra
    - Main config file contains default training configuration (see [config.yaml](project/configs/config.yaml))
    - Storing many experiment configurations in a convenient way (see [project/configs/experiment](project/configs/experiment))
    - Command line features (see [#How to run](README.md#How-to-run) for examples)
        - Override any config parameter from command line
        - Schedule execution of many experiments from command line
        - Sweep over hyperparameters from command line
    - Convenient logging of run history, ckpts, etc. (see [#Logs](README.md#Logs))
    - ~~Validating correctness of config with schemas~~ (TODO) 
- Optional Weights&Biases utilities for experiment tracking
    - Callbacks (see [wandb_callbacks.py](project/src/callbacks/wandb_callbacks.py))
        - Automatically store all code files and model checkpoints as artifacts in W&B cloud
        - Generate confusion matrices and f1/precision/recall heatmaps
    - ~~Hyperparameter search with Weights&Biases sweeps ([execute_sweep.py](project/template_utils/execute_sweep.py))~~ (TODO)
- Example of inference with trained model  ([inference_example.py](project/src/utils/inference_example.py))
- Built in requirements ([requirements.txt](requirements.txt))
- Built in conda environment initialization ([conda_env.yaml](conda_env.yaml))
- Built in python package setup ([setup.py](setup.py))
- Example with MNIST digits classification ([mnist_model.py](project/src/models/mnist_model.py), [mnist_datamodule.py](project/src/datamodules/mnist_datamodule.py))
<br>


## Project structure
The directory structure of new project looks like this: 
```
â”œâ”€â”€ project
â”‚   â”œâ”€â”€ configs                 <- Hydra configuration files
â”‚   â”‚   â”œâ”€â”€ trainer                 <- Configurations of lightning trainers
â”‚   â”‚   â”œâ”€â”€ model                   <- Configurations of lightning models
â”‚   â”‚   â”œâ”€â”€ datamodule              <- Configurations of lightning datamodules
â”‚   â”‚   â”œâ”€â”€ callbacks               <- Configurations of lightning callbacks
â”‚   â”‚   â”œâ”€â”€ logger                  <- Configurations of lightning loggers
â”‚   â”‚   â”œâ”€â”€ seeds                   <- Configurations of seeds
â”‚   â”‚   â”œâ”€â”€ experiment              <- Configurations of experiments
â”‚   â”‚   â”‚         
â”‚   â”‚   â””â”€â”€ config.yaml             <- Main project configuration file 
â”‚   â”‚
â”‚   â”œâ”€â”€ data                    <- Project data
â”‚   â”‚
â”‚   â”œâ”€â”€ logs                    <- Logs generated by hydra and pytorch lightning loggers
â”‚   â”‚
â”‚   â”œâ”€â”€ notebooks               <- Jupyter notebooks
â”‚   â”‚
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ architectures           <- PyTorch model architectures
â”‚   â”‚   â”œâ”€â”€ callbacks               <- PyTorch Lightning callbacks
â”‚   â”‚   â”œâ”€â”€ datamodules             <- PyTorch Lightning datamodules
â”‚   â”‚   â”œâ”€â”€ datasets                <- PyTorch datasets
â”‚   â”‚   â”œâ”€â”€ models                  <- PyTorch Lightning models
â”‚   â”‚   â”œâ”€â”€ transforms              <- Data transformations
â”‚   â”‚   â””â”€â”€ utils                   <- Utility scripts
â”‚   â”‚       â”œâ”€â”€ inference_example.py    <- Example of inference with trained model 
â”‚   â”‚       â””â”€â”€ template_utils.py       <- Some extra template utilities
â”‚   â”‚
â”‚   â””â”€â”€ train.py                <- Train model with chosen experiment configuration
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ conda_env.yaml          <- File for installing conda environment
â”œâ”€â”€ requirements.txt        <- File for installing python dependencies
â””â”€â”€ setup.py                <- File for installing project as a package
```
<br>


## Workflow
1. Write your PyTorch Lightning model (see [mnist_model.py](project/src/models/mnist_model.py) for example)
2. Write your PyTorch Lightning datamodule (see [mnist_datamodule.py](project/src/datamodules/mnist_datamodule.py) for example)
3. Write your experiment config, containing paths to your model and datamodule (see [project/configs/experiment](project/configs/experiment) for examples)
4. Run training with chosen experiment config:<br>
    ```bash
    python train.py +experiment=experiment_name.yaml
    ```
<br>


## Main project configuration file ([config.yaml](project/configs/config.yaml))
Main config contains default training configuration.<br>
It determines how config is composed when simply executing command: `python train.py`
```yaml
# to execute run with default training configuration simply run: 
# python train.py


# specify here default training configuration
defaults:
    - trainer: default_trainer.yaml
    - model: mnist_model.yaml
    - datamodule: mnist_datamodule.yaml
    - seeds: default_seeds.yaml  # set this to null if you don't want to use seeds
    - callbacks: default_callbacks.yaml  # set this to null if you don't want to use callbacks
    - logger: null  # set logger here or use command line (e.g. `python train.py logger=wandb`)


# path to original working directory (the directory that `train.py` was executed from in command line)
# hydra hijacks working directory by changing it to the current log directory,
# so it's useful to have path to original working directory as a special variable
# read more here: https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
original_work_dir: ${hydra:runtime.cwd}


# path to folder with data
data_dir: ${original_work_dir}/data/


# output paths for hydra logs
hydra:
    run:
        dir: logs/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    sweep:
        dir: logs/multiruns/${now:%Y-%m-%d_%H-%M-%S}
        subdir: ${hydra.job.num}
```
<br>


## Experiment configuration ([project/configs/experiment](project/configs/experiment))
You can store many experiment configurations in this folder.<br>
Example experiment configuration:
```yaml
# to execute this experiment run:
# python train.py +experiment=exp_example_simple

defaults:
    - override /trainer: default_trainer.yaml
    - override /model: mnist_model.yaml
    - override /datamodule: mnist_datamodule.yaml
    - override /seeds: default_seeds.yaml
    - override /callbacks: default_callbacks.yaml
    - override /logger: null

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seeds:
    pytorch_seed: 12345

trainer:
    max_epochs: 10
    gradient_clip_val: 0.5

model:
    lr: 0.001
    lin1_size: 128
    lin2_size: 256
    lin3_size: 64

datamodule:
    batch_size: 64
    train_val_test_split: [55_000, 5_000, 10_000]
```
<br>

More advanced experiment configuration:
```yaml
# to execute this experiment run:
# python train.py +experiment=exp_example_with_paths

defaults:
    - override /trainer: null
    - override /model: null
    - override /datamodule: null 
    - override /seeds: null
    - override /callbacks: default_callbacks.yaml
    - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config, 
# so everything is stored in one place for more readibility

seeds:
    pytorch_seed: 12345

trainer:
    _target_: pytorch_lightning.Trainer
    min_epochs: 1
    max_epochs: 10
    gradient_clip_val: 0.5

model:
    _target_: src.models.mnist_model.LitModelMNIST
    optimizer: adam
    lr: 0.001
    weight_decay: 0.000001
    architecture: SimpleDenseNet
    input_size: 784
    lin1_size: 256
    dropout1: 0.30
    lin2_size: 256
    dropout2: 0.25
    lin3_size: 128
    dropout3: 0.20
    output_size: 10

datamodule:
    _target_: src.datamodules.mnist_datamodule.MNISTDataModule
    data_dir: ${data_dir}
    batch_size: 64
    train_val_test_split: [55_000, 5_000, 10_000]
    num_workers: 1
    pin_memory: False
```
<br>


## Logs
By default, logs have the following structure:
```
â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ runs                     # Folder for logs generated from single runs
â”‚   â”‚   â”œâ”€â”€ 2021-02-15              # Date of executing run
â”‚   â”‚   â”‚   â”œâ”€â”€ 16-50-49                # Hour of executing run
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ .hydra                  # Hydra logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ wandb                   # Weights&Biases logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoints             # Training checkpoints
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...                     # Any other thing saved during training
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ multiruns               # Folder for logs generated from sweeps
â”‚   â”‚   â”œâ”€â”€ 2021-02-15_16-50-49     # Date and hour of executing sweep
â”‚   â”‚   â”‚   â”œâ”€â”€ 0                       # Job number
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ .hydra                  # Hydra logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ wandb                   # Weights&Biases logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoints             # Training checkpoints
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...                     # Any other thing saved during training
â”‚   â”‚   â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â”‚   â”œâ”€â”€ 2
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
```
<br><br>


### DELETE EVERYTHING ABOVE FOR YOUR PROJECT  
 
---

<div align="center">    
 
# Your Project Name     

[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
[![Conference](http://img.shields.io/badge/NeurIPS-2019-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)
[![Conference](http://img.shields.io/badge/ICLR-2019-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)
[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)  

</div>

## Description
What it does

## How to run
First, install dependencies:
```yaml
# clone project
git clone https://github.com/YourGithubName/your-repo-name
cd your-repo-name

# optionally create conda environment
conda update conda
conda env create -f conda_env.yaml -n your_env_name
conda activate your_env_name

# install requirements
pip install -r requirements.txt
```

Next, you can train model with default configuration without logging:
```yaml
cd project
python train.py
```

Or you can train model with chosen logger like Weights&Biases:
```yaml
# set project and entity names in 'project/configs/logger/wandb.yaml'
wandb:
    project: "your_project_name"
    entity: "your_wandb_team_name"
```

```yaml
# train model with Weights&Biases
python train.py logger=wandb
```

Or you can train model with chosen experiment config:
```yaml
# experiment configurations are placed in 'project/configs/experiment' folder
python train.py +experiment=exp_example_simple
```

To execute all experiments from folder run:
```yaml
# execute all experiments from folder `project/configs/experiment`
python train.py --multirun '+experiment=glob(*)'
```

You can override any parameter from command line like this:
```yaml
python train.py trainer.max_epochs=20 model.lr=0.0005
```

To train on GPU:
```yaml
python train.py trainer.gpus=1
```

Attach some callback set to run:
```yaml
# callback sets configurations are placed in 'project/configs/callbacks' folder
python train.py callbacks=default_callbacks
```

Combaining it all:
```yaml
python train.py --multirun '+experiment=glob(*)' trainer.max_epochs=10 logger=wandb
```

To create a sweep over some hyperparameters run:
```yaml
# this will run 6 experiments one after the other, 
# each with different combination of batch_size and learning rate
python train.py --multirun datamodule.batch_size=32,64,128 model.lr=0.001,0.0005
```

## Installing project as a package
Optionally you can install project as a package with [setup.py](setup.py):
```yaml
pip install -e .
```
So you can easily import any file into any other file like so:
```python
from project.src.datasets.img_test_dataset import TestDataset
from project.src.models.mnist_model import LitModelMNIST
from project.src.datamodules.mnist_datamodule import MNISTDataModule
```
