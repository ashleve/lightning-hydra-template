## Deep learning project template
This is my starting template and pipeline for most deep learning projects ðŸ”¥<br>
Built with <b>PyTorch Lightning</b>, <b>Hydra</b> and <b>Weights&Biases</b>.<br>
It's supposed to be enchancement/expansion on original [deep-learninig-project-template](https://github.com/PyTorchLightning/deep-learning-project-template) repository.<br>
The goal is to:
- structure ML code the same so that work can easily be extended and replicated
- allow for quick and efficient experimentation process thanks to automating pipeline with config files
- extend functionality of popular experiment loggers like Weights&Biases

Click on <b>`Use this template`</b> button above to initialize new repository.<br>

## Features
- Predefined folder structure
- Automates training with PyTorch Lightning
    - You only need to create model and datamodule and specify them in configuration files
- All advantages of Hydra
    - Main config file contains default training configuration ([config.yaml](project/configs/config.yaml))
    - Storing many experiment configurations in a convenient way ([configs/experiment](project/configs/experiment))
    - Composing configuration files out of other configuration files
    - Scheduling execution of many experiments
    - Overriding any config parameter from command line
    - Command line tab completion
    - Logging history of all executed runs/experiments
- Weights&Biases integration
    - Available callbacks that store all code files and model checkpoints as artifacts in Weights&Biases cloud ([wandb_callbacks.py](project/src/callbacks/wandb_callbacks.py))
    - Other examples of useful wandb callbacks ([wandb_callbacks.py](project/src/callbacks/wandb_callbacks.py))
    - Configuring which hyperparameters are saved by loggers through main config file ([config.yaml](project/configs/config.yaml))
    - ~~Hyperparameter search with Weights&Biases sweeps ([execute_sweep.py](project/template_utils/execute_sweep.py))~~ (TODO)
- Built in requirements ([requirements.txt](requirements.txt))
- Built in conda environment initialization ([conda_env.yaml](conda_env.yaml))
- Built in python package setup ([setup.py](setup.py))
- Example with MNIST digits classification ([mnist_model](project/src/models/mnist_model.py), [mnist_datamodule](project/src/datamodules/mnist_datamodule.py))
<br>


## Project structure
The directory structure of new project looks like this: 
```
â”œâ”€â”€ project
â”‚   â”œâ”€â”€ configs                 <- Hydra configuration files
â”‚   â”‚   â”œâ”€â”€ trainer                 <- Configurations of lightning trainers
â”‚   â”‚   â”œâ”€â”€ model                   <- Configurations of lightning models
â”‚   â”‚   â”œâ”€â”€ datamodule              <- Configurations of lightning datamodules
â”‚   â”‚   â”œâ”€â”€ callbacks               <- Configurations of lightning callbacks
â”‚   â”‚   â”œâ”€â”€ logger                  <- Configurations of lightning loggers
â”‚   â”‚   â”œâ”€â”€ optimizer               <- Configurations of optimizers
â”‚   â”‚   â”œâ”€â”€ seeds                   <- Configurations of seeds
â”‚   â”‚   â”œâ”€â”€ experiment              <- Configurations of experiments
â”‚   â”‚   â”‚         
â”‚   â”‚   â””â”€â”€ config.yaml             <- Main project configuration file 
â”‚   â”‚
â”‚   â”œâ”€â”€ data                    <- Project data
â”‚   â”‚
â”‚   â”œâ”€â”€ logs                    <- Logs generated by hydra and pytorch lightning loggers
â”‚   â”‚
â”‚   â”œâ”€â”€ notebooks               <- Jupyter notebooks
â”‚   â”‚
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ architectures           <- PyTorch model architectures
â”‚   â”‚   â”œâ”€â”€ callbacks               <- PyTorch Lightning callbacks
â”‚   â”‚   â”œâ”€â”€ datamodules             <- PyTorch Lightning datamodules
â”‚   â”‚   â”œâ”€â”€ datasets                <- PyTorch datasets
â”‚   â”‚   â”œâ”€â”€ models                  <- PyTorch Lightning models
â”‚   â”‚   â”œâ”€â”€ transforms              <- Data transformations
â”‚   â”‚   â””â”€â”€ utils                   <- Utility scripts
â”‚   â”‚       â”œâ”€â”€ inference_example.py    <- Example of inference with trained model 
â”‚   â”‚       â””â”€â”€ initializers.py         <- Initializers for different modules
â”‚   â”‚
â”‚   â””â”€â”€ train.py                <- Train model with chosen experiment configuration
â”‚
â”œâ”€â”€ .gitignore              <- Ignored files
â”œâ”€â”€ LICENSE                 <- Project license
â”œâ”€â”€ README.md               <- Readme
â”œâ”€â”€ conda_env.yaml          <- File for installing conda environment
â”œâ”€â”€ requirements.txt        <- List of python dependencies
â””â”€â”€ setup.py                <- File for installing project as a package
```
<br>


## Main project configuration file ([config.yaml](project/configs/config.yaml))
```yaml
# specify here default training configuration
defaults:
    - trainer: default_trainer.yaml         # choose trainer from 'configs/trainer/' folder
    - model: mnist_model.yaml   # choose model from 'configs/model/' folder
    - datamodule: mnist_datamodule.yaml     # choose datamodule from 'configs/datamodule/' folder
    - optimizer: adam.yaml                  # choose optimizer from 'configs/optimizer/' folder
    - seeds: default_seeds.yaml             # set this to null if you don't want to use seeds
    - callbacks: default_callbacks.yaml     # set this to null if you don't want to use callbacks
    - logger: null                          # set this to null if you don't want to use loggers

# path to working directory (the directory that `train.py` was executed from in command line)
work_dir: ${hydra:runtime.cwd}

# path to folder with data
data_dir: ${work_dir}/data/

# hydra output paths
hydra:
    run:
        dir: logs/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    sweep:
        dir: logs/multiruns/${now:%Y-%m-%d_%H-%M-%S}
        subdir: ${hydra.job.num}

# extra things that are logged by all loggers as hyperparameters
extra_logs:
    log_seeds: True

    log_trainer_args: False
    log_datamodule_args: True

    log_model_class: True
    log_optimizer_class: True
    log_datamodule_class: True
    log_model_architecture_class: True

    log_train_val_test_sizes: False
```
<br>


## Experiment configuration ([configs/experiment](project/configs/experiment))
You can store many experiment configurations in this folder.<br>
Example experiment configuration:
```yaml
defaults:
    - override /trainer: default_trainer.yaml           # choose trainer from 'configs/trainer/'
    - override /model: mnist_model.yaml     # choose model from 'configs/model/'
    - override /datamodule: mnist_datamodule.yaml       # choose datamodule from 'configs/datamodule/'
    - override /optimizer: adam.yaml                    # choose optimizer from 'configs/optimizer/'
    - override /seeds: default_seeds.yaml               # choose seeds from 'configs/seeds/'
    - override /callbacks: default_callbacks.yaml       # choose callback set from 'configs/callbacks/'
    - override /logger: null                            # choose logger from 'configs/logger/'

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seeds:
    pytorch_seed: 12345

trainer:
    args:
        max_epochs: 10

model:
    args:
        input_size: 784
        output_size: 10
        lin1_size: 256
        lin2_size: 256
        lin3_size: 128

datamodule:
    args:
        batch_size: 32
        train_val_test_split: [55_000, 5_000, 10_000]

optimizer:
    args:
        lr: 0.001
        weight_decay: 0.00001
```
To execute this experiment run:
```bash
python train.py +experiment=exp_example_simple
```
<br>


## Logs
Logs are created automatically with the following structure:
```
â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ runs                     # Folder for logs generated from single runs
â”‚   â”‚   â”œâ”€â”€ 2021-02-15              # Date of executing run
â”‚   â”‚   â”‚   â”œâ”€â”€ 16-50-49                # Hour of executing run
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ .hydra                  # Hydra logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ wandb                   # Weights&Biases logs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ checkpoints             # Training checkpoints
â”‚   â”‚   â”‚   â”‚   
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ multiruns               # Folder for logs generated from sweeps
â”‚   â”‚   â”œâ”€â”€ 2021-02-15_16-50-49     # Date and hour of executing run
â”‚   â”‚   â”‚   â”œâ”€â”€ 0                       # Job number
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ .hydra                  # Hydra logs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ wandb                   # Weights&Biases logs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ checkpoints             # Training checkpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â”‚   â”œâ”€â”€ 2
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
```
<br>


## Workflow
1. Create PyTorch Lightning model
2. Create PyTorch Lightning datamodule
3. Create new experiment config in [configs/experiment](project/configs/experiment) folder
4. Run training with chosen experiment config<br>
    ```bash
    python train.py +experiment=experiment_name.yaml
    ```
<br><br>


### DELETE EVERYTHING ABOVE FOR YOUR PROJECT  
 
---

<div align="center">    
 
# Your Project Name     

[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
[![Conference](http://img.shields.io/badge/NeurIPS-2019-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)
[![Conference](http://img.shields.io/badge/ICLR-2019-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)
[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)  

</div>

## Description
What it does

## How to run
First, install dependencies
```bash
# clone project
git clone https://github.com/YourGithubName/your-repo-name
cd your-repo-name

# optionally create conda environment
conda update conda
conda env create -f conda_env.yaml -n your_env_name
conda activate your_env_name

# install requirements
pip install -r requirements.txt

pip install hydra-core --upgrade --pre
```

Next, you can train model with default configuration without logging
```bash
cd project
python train.py
```

Or you can train model with chosen logger like Weights&Biases
```yaml
# set project and entity names in project/configs/logger/wandb.yaml
wandb:
    args:
        project: "your_project_name"
        entity: "your_wandb_team_name"
```
```bash
# train model with Weights&Biases
python train.py logger=wandb.yaml
```

Or you can train model with chosen experiment config
```bash
python train.py +experiment=exp_example_simple.yaml
```

Optionally you can install project as a package with [setup.py](setup.py)
```bash
pip install -e .
```
<br>


#### PyCharm setup
- open this repository as PyCharm project
- set project interpreter:<br> 
`Ctrl + Shift + A -> type "Project Interpreter"`
- mark folder "project" as sources root:<br>
`right click on directory -> "Mark Directory as" -> "Sources Root"`
- set terminal emulation:<br> 
`Ctrl + Shift + A -> type "Edit Configurations..." -> select "Emulate terminal in output console"`
- run training:<br>
`right click on train.py file -> "Run 'train'"`


#### VS Code setup
- TODO
