# @package _global_

# to execute this experiment run:
# python train.py +experiment=exp_example_advanced

defaults:
    - override /trainer: default_trainer.yaml           # choose trainer from 'configs/trainer/' folder
    - override /model: mnist_model.yaml                 # choose model from 'configs/model/' folder
    - override /datamodule: mnist_datamodule.yaml       # choose datamodule from 'configs/datamodule/' folder
    - override /seeds: default_seeds.yaml               # choose seeds from 'configs/seeds/' folder
    - override /callbacks: default_callbacks.yaml       # choose callback set from 'configs/callbacks/' folder
    - override /logger: null                            # choose logger from 'configs/logger/' folder or set it from console when running experiment:
                                                        # `python train.py +experiment=exp_example_advanced logger=wandb`

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seeds:
    pytorch_seed: 12345                             # pytorch seed for this experiment (affects torch.utils.data.random_split() method used in mnist_datamodule)

trainer:
    min_epochs: 1                                   # train for at least this many epochs (denies early stopping)
    max_epochs: 10                                  # train for maximum this many epochs
    gradient_clip_val: 0.5                          # gradient clipping (helps with exploding gradient issues)
    accumulate_grad_batches: 2                      # perform optimization step after accumulating gradient from 2 batches
    fast_dev_run: False                             # execute 1 training, 1 validation and 1 test epoch only
    limit_train_batches: 0.6                        # train on 60% of training data
    limit_val_batches: 0.9                          # validate on 90% of validation data
    limit_test_batches: 1.0                         # test on 100% of test data
    val_check_interval: 0.5                         # perform validation twice per epoch
    # resume_from_checkpoint: ${work_dir}/last.ckpt  # path to checkpoint (this can be also url for download)

model:                                         # you can add here any params you want and then access them in lightning model
    lr: 0.001
    weight_decay: 0.00001
    input_size: 784                                 # img size is 1*28*28
    output_size: 10                                 # there are 10 digit classes
    lin1_size: 256
    lin2_size: 256
    lin3_size: 128

datamodule:                                     # you can add here any params you want and then access them in lightning datamodule
    batch_size: 64
    train_val_test_split: [55_000, 5_000, 10_000]
    num_workers: 1                                  # num of processes used for loading data in parallel
    pin_memory: False                               # dataloaders will copy tensors into CUDA pinned memory before returning them

logger:                                         # you can add here additional logger arguments specific for this experiment
    wandb:
        tags: ["best_model", "uwu"]
        notes: "Description of this model."
        group: "mnist"
    neptune:
        tags: ["best_model"]
    csv_logger:
        save_dir: "."
