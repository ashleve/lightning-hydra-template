# @package _global_

# to execute this experiment run:
# python train.py +experiment=exp_example_advanced

defaults:
    - override /trainer: default_trainer.yaml           # choose trainer from 'configs/trainer/' folder
    - override /model: mnist_model.yaml                 # choose model from 'configs/model/' folder
    - override /datamodule: mnist_datamodule.yaml       # choose datamodule from 'configs/datamodule/' folder
    - override /optimizer: adam.yaml                    # choose optimizer from 'configs/optimizer/' folder
    - override /seeds: default_seeds.yaml               # choose seeds from 'configs/seeds/' folder
    - override /callbacks: default_callbacks.yaml       # choose callback set from 'configs/callbacks/' folder
    - override /logger: null                            # choose logger from 'configs/logger/' folder

seeds:
    pytorch_seed: 12345                                 # pytorch seed for this experiment (affects torch.utils.data.random_split() method used in mnist_datamodule)

trainer:
    args:
        min_epochs: 1                                   # train for at least this many epochs (denies early stopping)
        max_epochs: 10                                  # train for maximum this many epochs
        gradient_clip_val: 0.5                          # gradient clipping (helps with exploding gradient issues)
        accumulate_grad_batches: 2                      # perform optimization step after accumulating gradient from 2 batches
        fast_dev_run: False                             # execute 1 training, 1 validation and 1 test epoch only
        limit_train_batches: 0.6                        # train on 60% of training data
        limit_val_batches: 0.9                          # validate on 90% of validation data
        limit_test_batches: 1.0                         # test on 100% of test data
        val_check_interval: 0.5                         # perform validation twice per epoch
        # resume_from_checkpoint: ${work_dir}/last.ckpt  # path to checkpoint (this can be also url for download)

model:
    args:                                               # you can add here any params you want and then access them in lightning model
        input_size: 784                                 # img size is 1*28*28
        output_size: 10                                 # there are 10 digit classes
        lin1_size: 256
        lin2_size: 256
        lin3_size: 128

datamodule:
    args:                                               # you can add here any params you want and then access them in lightning datamodule
        batch_size: 64
        train_val_test_split: [55_000, 5_000, 10_000]
        num_workers: 1                                  # num of processes used for loading data in parallel
        pin_memory: False                               # dataloaders will copy tensors into CUDA pinned memory before returning them

optimizer:
    args:
        lr: 0.001
        weight_decay: 0.00001

logger:
    wandb:
        args:                                           # you can add here additional logger arguments specific for this experiment
            tags: ["best_model", "uwu"]
            notes: "Description of this model."
            group: ""
    comet:
        args:
            tags: ["best_model"]
