# Trainer args for debugging model

_target_: pytorch_lightning.Trainer


gpus: 0

min_epochs: 1
max_epochs: 3

# prints
progress_bar_refresh_rate: null
weights_summary: null
profiler: null

# all parameters below are set to Lightning defaults

# tricks
gradient_clip_val: 0
accumulate_grad_batches: 1
val_check_interval: 1.0
check_val_every_n_epoch: 1
stochastic_weight_avg: False

# debug
num_sanity_val_steps: 2
fast_dev_run: False
overfit_batches: 0
limit_train_batches: 1.0
limit_val_batches: 1.0
limit_test_batches: 1.0
track_grad_norm: -1
terminate_on_nan: False

# others
amp_backend: "native"
amp_level: "02"
precision: 32
accelerator: null
num_nodes: 1
tpu_cores: null
deterministic: False

resume_from_checkpoint: null


# automatically find the largest batch size that fits into memory and is power of 2
# (requires calling trainer.tune(model=model, datamodule=datamodule))
# auto_scale_batch_size:'power'
