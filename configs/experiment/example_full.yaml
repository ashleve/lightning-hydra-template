# @package _global_

# to execute this experiment run:
# python run.py experiment=example_full.yaml

defaults:
  - override /mode: exp.yaml
  - override /trainer: null
  - override /model: null
  - override /datamodule: null
  - override /callbacks: null
  - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

# name of the run determines folder name in logs
# it's also accessed by loggers
name: "example_full"

seed: 12345

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 0
  min_epochs: 1
  max_epochs: 10
  gradient_clip_val: 0.5
  accumulate_grad_batches: 2
  weights_summary: "full"
  num_sanity_val_steps: 0

model:
  _target_: src.models.mnist_model.MNISTLitModel
  input_size: 784
  lin1_size: 256
  lin2_size: 256
  lin3_size: 128
  output_size: 10
  lr: 0.001
  weight_decay: 0.0005

datamodule:
  _target_: src.datamodules.mnist_datamodule.MNISTDataModule
  data_dir: ${data_dir}
  batch_size: 64
  train_val_test_split: [55_000, 5_000, 10_000]
  num_workers: 0
  pin_memory: False

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/acc"
    mode: "max"
    save_top_k: 1
    save_last: True
    verbose: False
    dirpath: "checkpoints/"
    filename: "epoch_{epoch:03d}"
    auto_insert_metric_name: False
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/acc"
    mode: "max"
    patience: 5
    min_delta: 0
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar

logger:
  wandb:
    name: ${name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    tags: ["best_model", "mnist"]
    notes: "Description of this model."
