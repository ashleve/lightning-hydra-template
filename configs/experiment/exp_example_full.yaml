# @package _global_

# to execute this experiment run:
# python run.py +experiment=exp_example_full

defaults:
    - override /trainer: null  # override trainer to null so it's not loaded from main config defaults...
    - override /model: null
    - override /optimizer: null
    - override /datamodule: null
    - override /callbacks: null
    - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place for more readibility

seed: 12345

trainer:
    _target_: pytorch_lightning.Trainer
    gpus: 0
    min_epochs: 1
    max_epochs: 10
    gradient_clip_val: 0.5
    accumulate_grad_batches: 2
    weights_summary: null
    # resume_from_checkpoint: ${work_dir}/last.ckpt

model:
    _target_: src.pl_models.mnist_model.MNISTLitModel
    optimizer: adam
    lr: 0.001
    weight_decay: 0.00005
    architecture: SimpleDenseNet
    input_size: 784
    lin1_size: 256
    lin2_size: 256
    lin3_size: 128
    output_size: 10

optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
    weight_decay: 0

datamodule:
    _target_: src.pl_datamodules.mnist_datamodule.MNISTDataModule
    data_dir: ${data_dir}
    batch_size: 64
    train_val_test_split: [55_000, 5_000, 10_000]
    num_workers: 0
    pin_memory: False

callbacks:
    model_checkpoint:
        _target_: pytorch_lightning.callbacks.ModelCheckpoint
        monitor: "val/acc"
        save_top_k: 2
        save_last: True
        mode: "max"
        dirpath: 'checkpoints/'
        filename: 'sample-mnist-{epoch:02d}'
    early_stopping:
        _target_: pytorch_lightning.callbacks.EarlyStopping
        monitor: "val/acc"
        patience: 10
        mode: "max"

logger:
    wandb:
        tags: ["best_model", "uwu"]
        notes: "Description of this model."
    neptune:
        tags: ["best_model"]
    csv_logger:
        save_dir: "."
