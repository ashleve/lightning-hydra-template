/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_hip.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
[[36m2023-03-02 21:46:03,576[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-03-02 21:46:03,581[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamod
â”‚       db_path: /groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Dat
â”‚       event_no_list_path: /groups/icecube/moust/storage/event_selections/event
â”‚       pulsemap: SplitInIcePulses_dynedge_v2_Pulses                            
â”‚       input_cols:                                                             
â”‚       - charge                                                                
â”‚       - dom_time                                                              
â”‚       - dom_x                                                                 
â”‚       - dom_y                                                                 
â”‚       - dom_z                                                                 
â”‚       - pmt_dir_x                                                             
â”‚       - pmt_dir_y                                                             
â”‚       - pmt_dir_z                                                             
â”‚       target_cols: energy                                                     
â”‚       truth_table: truth                                                      
â”‚       data_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/data/ 
â”‚       batch_size: 32                                                          
â”‚       num_workers: 16                                                         
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.simple_transformer_encoder_pooling_module.SimpleTra
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       model:                                                                  
â”‚         _target_: src.models.components.simple_transformer_encoder_pooling.Sim
â”‚         input_size: 8                                                         
â”‚         d_model: 16                                                           
â”‚         nhead: 2                                                              
â”‚         dim_feedforward: 256                                                  
â”‚         dropout: 0.1                                                          
â”‚         num_layers: 1                                                         
â”‚         output_size: 1                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: lightning-hydra-template                                     
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         entity: graphnet-team                                                 
â”‚         group: transformer                                                    
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydr
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 10                                                          
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra       
â”‚       data_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/data/ 
â”‚       log_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/  
â”‚       output_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs
â”‚       work_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra       
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['dev']                                                                 
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ None                                                                    
[[36m2023-03-02 21:46:03,641[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamodule>[0m
[[36m2023-03-02 21:46:05,025[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.simple_transformer_encoder_pooling_module.SimpleTransformerEncoderPoolingLitModule>[0m
[[36m2023-03-02 21:46:05,070[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp41nq9_a3[0m
[[36m2023-03-02 21:46:05,070[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp41nq9_a3/_remote_module_non_sriptable.py[0m
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
[[36m2023-03-02 21:46:05,101[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-03-02 21:46:05,102[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2023-03-02 21:46:05,105[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2023-03-02 21:46:05,105[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2023-03-02 21:46:05,106[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2023-03-02 21:46:05,106[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-03-02 21:46:05,106[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: moustholmes (graphnet-team). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-02_21-46-03/wandb/run-20230302_214608-9nddi5rl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-lion-2
wandb: â­ï¸ View project at https://wandb.ai/graphnet-team/lightning-hydra-template
wandb: ğŸš€ View run at https://wandb.ai/graphnet-team/lightning-hydra-template/runs/9nddi5rl
[[36m2023-03-02 21:46:14,298[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python src/train.py trainer=gpu ...
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-03-02 21:46:14,362[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-03-02 21:46:14,368[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                                                  â”ƒ Type   â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model                                                 â”‚ Simplâ€¦ â”‚ 19.4 K â”‚
â”‚ 1  â”‚ model.fc_in                                           â”‚ Linear â”‚    144 â”‚
â”‚ 2  â”‚ model.encoder_layer                                   â”‚ Transâ€¦ â”‚  9.6 K â”‚
â”‚ 3  â”‚ model.encoder_layer.self_attn                         â”‚ Multiâ€¦ â”‚  1.1 K â”‚
â”‚ 4  â”‚ model.encoder_layer.self_attn.out_proj                â”‚ NonDyâ€¦ â”‚    272 â”‚
â”‚ 5  â”‚ model.encoder_layer.linear1                           â”‚ Linear â”‚  4.4 K â”‚
â”‚ 6  â”‚ model.encoder_layer.dropout                           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 7  â”‚ model.encoder_layer.linear2                           â”‚ Linear â”‚  4.1 K â”‚
â”‚ 8  â”‚ model.encoder_layer.norm1                             â”‚ Layerâ€¦ â”‚     32 â”‚
â”‚ 9  â”‚ model.encoder_layer.norm2                             â”‚ Layerâ€¦ â”‚     32 â”‚
â”‚ 10 â”‚ model.encoder_layer.dropout1                          â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 11 â”‚ model.encoder_layer.dropout2                          â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 12 â”‚ model.transformer_encoder                             â”‚ Transâ€¦ â”‚  9.6 K â”‚
â”‚ 13 â”‚ model.transformer_encoder.layers                      â”‚ Modulâ€¦ â”‚  9.6 K â”‚
â”‚ 14 â”‚ model.transformer_encoder.layers.0                    â”‚ Transâ€¦ â”‚  9.6 K â”‚
â”‚ 15 â”‚ model.transformer_encoder.layers.0.self_attn          â”‚ Multiâ€¦ â”‚  1.1 K â”‚
â”‚ 16 â”‚ model.transformer_encoder.layers.0.self_attn.out_proj â”‚ NonDyâ€¦ â”‚    272 â”‚
â”‚ 17 â”‚ model.transformer_encoder.layers.0.linear1            â”‚ Linear â”‚  4.4 K â”‚
â”‚ 18 â”‚ model.transformer_encoder.layers.0.dropout            â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 19 â”‚ model.transformer_encoder.layers.0.linear2            â”‚ Linear â”‚  4.1 K â”‚
â”‚ 20 â”‚ model.transformer_encoder.layers.0.norm1              â”‚ Layerâ€¦ â”‚     32 â”‚
â”‚ 21 â”‚ model.transformer_encoder.layers.0.norm2              â”‚ Layerâ€¦ â”‚     32 â”‚
â”‚ 22 â”‚ model.transformer_encoder.layers.0.dropout1           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 23 â”‚ model.transformer_encoder.layers.0.dropout2           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 24 â”‚ model.fc_out                                          â”‚ Linear â”‚     17 â”‚
â”‚ 25 â”‚ loss_fn                                               â”‚ MSELoâ€¦ â”‚      0 â”‚
â”‚ 26 â”‚ train_loss                                            â”‚ MeanMâ€¦ â”‚      0 â”‚
â”‚ 27 â”‚ val_loss                                              â”‚ MeanMâ€¦ â”‚      0 â”‚
â”‚ 28 â”‚ test_loss                                             â”‚ MeanMâ€¦ â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 19.4 K                                                        
Non-trainable params: 0                                                         
Total params: 19.4 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 0/9  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15267/15267 8:22:52 â€¢       1.11it/s loss: 1.04e+03  
                                       0:00:00                  v_num: i5rl     
                                                                val/loss:       
                                                                1987.679        
[[36m2023-03-03 06:11:58,263[0m][[34msrc.utils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/utils/utils.py", line 38, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/train.py", line 82, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1214, in _run_train
    self.fit_loop.run()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.on_advance_end()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 295, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1394, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 184, in on_train_epoch_end
    self._run_early_stopping_check(trainer)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 195, in _run_early_stopping_check
    if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 150, in _validate_condition_metric
    raise RuntimeError(error_msg)
RuntimeError: Early stopping conditioned on metric `val/acc` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `val/loss`, `train/loss`
[[36m2023-03-03 06:11:58,455[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-02_21-46-03[0m
[[36m2023-03-03 06:11:58,455[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2023-03-03 06:11:58,456[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch â–
wandb: trainer/global_step â–
wandb:            val/loss â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb: trainer/global_step 13576
wandb:            val/loss 1987.67883
wandb: 
wandb: ğŸš€ View run expert-lion-2 at: https://wandb.ai/graphnet-team/lightning-hydra-template/runs/9nddi5rl
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./logs/train/runs/2023-03-02_21-46-03/wandb/run-20230302_214608-9nddi5rl/logs
Error executing job with overrides: ['trainer=gpu']
Traceback (most recent call last):
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/train.py", line 107, in main
    metric_dict, _ = train(cfg)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/utils/utils.py", line 48, in wrap
    raise ex
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/utils/utils.py", line 38, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/train.py", line 82, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1214, in _run_train
    self.fit_loop.run()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.on_advance_end()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 295, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1394, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 184, in on_train_epoch_end
    self._run_early_stopping_check(trainer)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 195, in _run_early_stopping_check
    if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 150, in _validate_condition_metric
    raise RuntimeError(error_msg)
RuntimeError: Early stopping conditioned on metric `val/acc` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `val/loss`, `train/loss`

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_hip.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
[[36m2023-03-05 20:57:38,307[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-03-05 20:57:38,336[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamod
â”‚       db_path: /groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Dat
â”‚       event_no_list_path: /groups/icecube/moust/storage/event_selections/event
â”‚       input_cols:                                                             
â”‚       - charge                                                                
â”‚       - dom_time                                                              
â”‚       - dom_x                                                                 
â”‚       - dom_y                                                                 
â”‚       - dom_z                                                                 
â”‚       - pmt_dir_x                                                             
â”‚       - pmt_dir_y                                                             
â”‚       - pmt_dir_z                                                             
â”‚       target_cols: energy                                                     
â”‚       truth_table: truth                                                      
â”‚       data_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/data/ 
â”‚       batch_size: 32                                                          
â”‚       num_workers: 16                                                         
â”‚       pin_memory: FalseÂ¶                                                      
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.simple_transformer_encoder_pooling_module.SimpleTra
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       model:                                                                  
â”‚         _target_: src.models.components.simple_transformer_encoder_pooling.Sim
â”‚         input_size: 8                                                         
â”‚         d_model: 16                                                           
â”‚         nhead: 2                                                              
â”‚         dim_feedforward: 256                                                  
â”‚         dropout: 0.1                                                          
â”‚         num_layers: 1                                                         
â”‚         output_size: 1                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/acc                                                      
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: max                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: lightning-hydra-template                                     
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         entity: graphnet-team                                                 
â”‚         group: transformer                                                    
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydr
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 10                                                          
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra       
â”‚       data_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/data/ 
â”‚       log_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/  
â”‚       output_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs
â”‚       work_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra       
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['dev']                                                                 
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ None                                                                    
[[36m2023-03-05 20:57:38,409[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamodule>[0m
[[36m2023-03-05 20:57:44,984[0m][[34msrc.utils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 1 required positional argument: 'pulsemap'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/utils/utils.py", line 38, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/train.py", line 53, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/hydra/_internal/instantiate/_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamodule':
TypeError("__init__() missing 1 required positional argument: 'pulsemap'")
full_key: data
[[36m2023-03-05 20:57:45,008[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-05_20-57-38[0m
[[36m2023-03-05 20:57:45,009[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
Error executing job with overrides: ['trainer=gpu']
Error in call to target 'src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamodule':
TypeError("__init__() missing 1 required positional argument: 'pulsemap'")
full_key: data

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_hip.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
[[36m2023-03-07 10:07:11,403[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-03-07 10:07:11,432[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamod
â”‚       db_path: /groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Dat
â”‚       event_no_list_path: /groups/icecube/moust/storage/event_selections/event
â”‚       pulsemap: SplitInIcePulses_dynedge_v2_Pulses                            
â”‚       input_cols:                                                             
â”‚       - charge                                                                
â”‚       - dom_time                                                              
â”‚       - dom_x                                                                 
â”‚       - dom_y                                                                 
â”‚       - dom_z                                                                 
â”‚       - pmt_dir_x                                                             
â”‚       - pmt_dir_y                                                             
â”‚       - pmt_dir_z                                                             
â”‚       target_cols: energy                                                     
â”‚       truth_table: truth                                                      
â”‚       data_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/data/ 
â”‚       batch_size: 64                                                          
â”‚       num_workers: 16                                                         
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.simple_transformer_encoder_pooling_module.SimpleTra
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       model:                                                                  
â”‚         _target_: src.models.components.simple_transformer_encoder_pooling.Sim
â”‚         input_size: 5                                                         
â”‚         d_model: 16                                                           
â”‚         nhead: 2                                                              
â”‚         dim_feedforward: 256                                                  
â”‚         dropout: 0.1                                                          
â”‚         num_layers: 1                                                         
â”‚         output_size: 1                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/loss                                                     
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/loss                                                     
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: lightning-hydra-template                                     
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         entity: graphnet-team                                                 
â”‚         group: transformer                                                    
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydr
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 2                                                           
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra       
â”‚       data_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/data/ 
â”‚       log_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/  
â”‚       output_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs
â”‚       work_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra       
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['dev']                                                                 
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ None                                                                    
[[36m2023-03-07 10:07:11,555[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamodule>[0m
/groups/icecube/moust/storage/event_selections/event_no_numu_track_energy_15_200_nhits_4_400_sorted.csv
[[36m2023-03-07 10:07:18,259[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.simple_transformer_encoder_pooling_module.SimpleTransformerEncoderPoolingLitModule>[0m
[[36m2023-03-07 10:07:18,663[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpvb2i7pn6[0m
[[36m2023-03-07 10:07:18,664[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpvb2i7pn6/_remote_module_non_sriptable.py[0m
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
[[36m2023-03-07 10:07:18,949[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-03-07 10:07:18,950[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2023-03-07 10:07:18,952[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2023-03-07 10:07:18,953[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2023-03-07 10:07:18,953[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2023-03-07 10:07:18,954[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-03-07 10:07:18,954[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: moustholmes (graphnet-team). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-07_10-07-11/wandb/run-20230307_100722-vwhwwvw0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-puddle-8
wandb: â­ï¸ View project at https://wandb.ai/graphnet-team/lightning-hydra-template
wandb: ğŸš€ View run at https://wandb.ai/graphnet-team/lightning-hydra-template/runs/vwhwwvw0
[[36m2023-03-07 10:07:32,075[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python src/train.py trainer=gpu ...
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-03-07 10:07:32,135[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-03-07 10:07:32,140[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                                                  â”ƒ Type   â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model                                                 â”‚ Simplâ€¦ â”‚ 19.3 K â”‚
â”‚ 1  â”‚ model.fc_in                                           â”‚ Linear â”‚     96 â”‚
â”‚ 2  â”‚ model.encoder_layer                                   â”‚ Transâ€¦ â”‚  9.6 K â”‚
â”‚ 3  â”‚ model.encoder_layer.self_attn                         â”‚ Multiâ€¦ â”‚  1.1 K â”‚
â”‚ 4  â”‚ model.encoder_layer.self_attn.out_proj                â”‚ NonDyâ€¦ â”‚    272 â”‚
â”‚ 5  â”‚ model.encoder_layer.linear1                           â”‚ Linear â”‚  4.4 K â”‚
â”‚ 6  â”‚ model.encoder_layer.dropout                           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 7  â”‚ model.encoder_layer.linear2                           â”‚ Linear â”‚  4.1 K â”‚
â”‚ 8  â”‚ model.encoder_layer.norm1                             â”‚ Layerâ€¦ â”‚     32 â”‚
â”‚ 9  â”‚ model.encoder_layer.norm2                             â”‚ Layerâ€¦ â”‚     32 â”‚
â”‚ 10 â”‚ model.encoder_layer.dropout1                          â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 11 â”‚ model.encoder_layer.dropout2                          â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 12 â”‚ model.transformer_encoder                             â”‚ Transâ€¦ â”‚  9.6 K â”‚
â”‚ 13 â”‚ model.transformer_encoder.layers                      â”‚ Modulâ€¦ â”‚  9.6 K â”‚
â”‚ 14 â”‚ model.transformer_encoder.layers.0                    â”‚ Transâ€¦ â”‚  9.6 K â”‚
â”‚ 15 â”‚ model.transformer_encoder.layers.0.self_attn          â”‚ Multiâ€¦ â”‚  1.1 K â”‚
â”‚ 16 â”‚ model.transformer_encoder.layers.0.self_attn.out_proj â”‚ NonDyâ€¦ â”‚    272 â”‚
â”‚ 17 â”‚ model.transformer_encoder.layers.0.linear1            â”‚ Linear â”‚  4.4 K â”‚
â”‚ 18 â”‚ model.transformer_encoder.layers.0.dropout            â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 19 â”‚ model.transformer_encoder.layers.0.linear2            â”‚ Linear â”‚  4.1 K â”‚
â”‚ 20 â”‚ model.transformer_encoder.layers.0.norm1              â”‚ Layerâ€¦ â”‚     32 â”‚
â”‚ 21 â”‚ model.transformer_encoder.layers.0.norm2              â”‚ Layerâ€¦ â”‚     32 â”‚
â”‚ 22 â”‚ model.transformer_encoder.layers.0.dropout1           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 23 â”‚ model.transformer_encoder.layers.0.dropout2           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 24 â”‚ model.fc_out                                          â”‚ Linear â”‚     17 â”‚
â”‚ 25 â”‚ loss_fn                                               â”‚ MSELoâ€¦ â”‚      0 â”‚
â”‚ 26 â”‚ train_loss                                            â”‚ MeanMâ€¦ â”‚      0 â”‚
â”‚ 27 â”‚ val_loss                                              â”‚ MeanMâ€¦ â”‚      0 â”‚
â”‚ 28 â”‚ test_loss                                             â”‚ MeanMâ€¦ â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 19.3 K                                                        
Non-trainable params: 0                                                         
Total params: 19.3 K                                                            
Total estimated model params size (MB): 0                                       

[[36m2023-03-07 10:07:54,285[0m][[34msrc.utils.utils[0m][[31mERROR[0m] - [0m
Traceback (most recent call last):
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/utils/utils.py", line 38, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/train.py", line 82, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1204, in _run_train
    self._run_sanity_check()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1276, in _run_sanity_check
    val_loop.run()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 152, in advance
    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 137, in advance
    output = self._evaluation_step(**kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 234, in _evaluation_step
    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1494, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/models/simple_transformer_encoder_pooling_module.py", line 97, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/models/simple_transformer_encoder_pooling_module.py", line 65, in model_step
    preds = self.forward(x, pad_mask)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/models/simple_transformer_encoder_pooling_module.py", line 54, in forward
    return self.model(x,pad_mask)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/models/components/simple_transformer_encoder_pooling.py", line 32, in forward
    x = self.fc_in(x)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (12800x8 and 5x16)
[[36m2023-03-07 10:07:54,494[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-07_10-07-11[0m
[[36m2023-03-07 10:07:54,494[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2023-03-07 10:07:54,494[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)wandb: ğŸš€ View run rural-puddle-8 at: https://wandb.ai/graphnet-team/lightning-hydra-template/runs/vwhwwvw0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./logs/train/runs/2023-03-07_10-07-11/wandb/run-20230307_100722-vwhwwvw0/logs
Error executing job with overrides: ['trainer=gpu']
Traceback (most recent call last):
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/train.py", line 107, in main
    metric_dict, _ = train(cfg)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/utils/utils.py", line 48, in wrap
    raise ex
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/utils/utils.py", line 38, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/train.py", line 82, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1204, in _run_train
    self._run_sanity_check()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1276, in _run_sanity_check
    val_loop.run()
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 152, in advance
    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 137, in advance
    output = self._evaluation_step(**kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 234, in _evaluation_step
    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1494, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/models/simple_transformer_encoder_pooling_module.py", line 97, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/models/simple_transformer_encoder_pooling_module.py", line 65, in model_step
    preds = self.forward(x, pad_mask)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/models/simple_transformer_encoder_pooling_module.py", line 54, in forward
    return self.model(x,pad_mask)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/src/models/components/simple_transformer_encoder_pooling.py", line 32, in forward
    x = self.fc_in(x)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (12800x8 and 5x16)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_hip.so: cannot open shared object file: No such file or directory
  warn(f"Failed to load image Python extension: {e}")
[[36m2023-03-07 11:51:48,599[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2023-03-07 11:51:48,604[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ data
â”‚   â””â”€â”€ _target_: src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamod
â”‚       db_path: /groups/icecube/petersen/GraphNetDatabaseRepository/Upgrade_Dat
â”‚       event_no_list_path: /groups/icecube/moust/storage/event_selections/event
â”‚       pulsemap: SplitInIcePulses_dynedge_v2_Pulses                            
â”‚       input_cols:                                                             
â”‚       - charge                                                                
â”‚       - dom_time                                                              
â”‚       - dom_x                                                                 
â”‚       - dom_y                                                                 
â”‚       - dom_z                                                                 
â”‚       - pmt_dir_x                                                             
â”‚       - pmt_dir_y                                                             
â”‚       - pmt_dir_z                                                             
â”‚       target_cols: energy                                                     
â”‚       truth_table: truth                                                      
â”‚       data_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/data/ 
â”‚       batch_size: 256                                                         
â”‚       num_workers: 16                                                         
â”‚       pin_memory: false                                                       
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.simple_transformer_encoder_pooling_module.SimpleTra
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       scheduler:                                                              
â”‚         _target_: torch.optim.lr_scheduler.ReduceLROnPlateau                  
â”‚         _partial_: true                                                       
â”‚         mode: min                                                             
â”‚         factor: 0.1                                                           
â”‚         patience: 10                                                          
â”‚       model:                                                                  
â”‚         _target_: src.models.components.simple_transformer_encoder_pooling.Sim
â”‚         input_size: 8                                                         
â”‚         d_model: 32                                                           
â”‚         nhead: 2                                                              
â”‚         dim_feedforward: 256                                                  
â”‚         dropout: 0.1                                                          
â”‚         num_layers: 2                                                         
â”‚         output_size: 1                                                        
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/loss                                                     
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: min                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping:                                                         
â”‚         _target_: pytorch_lightning.callbacks.EarlyStopping                   
â”‚         monitor: val/loss                                                     
â”‚         min_delta: 0.0                                                        
â”‚         patience: 100                                                         
â”‚         verbose: false                                                        
â”‚         mode: min                                                             
â”‚         strict: true                                                          
â”‚         check_finite: true                                                    
â”‚         stopping_threshold: null                                              
â”‚         divergence_threshold: null                                            
â”‚         check_on_train_epoch_end: null                                        
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: lightning-hydra-template                                     
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         entity: graphnet-team                                                 
â”‚         group: transformer                                                    
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydr
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 30                                                          
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       check_val_every_n_epoch: 1                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra       
â”‚       data_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/data/ 
â”‚       log_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/  
â”‚       output_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs
â”‚       work_dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra       
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['dev']                                                                 
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â””â”€â”€ seed
    â””â”€â”€ None                                                                    
[[36m2023-03-07 11:51:48,648[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating datamodule <src.data.simple_icecube_SQL_datamodule.SimpleIceCubeSQLDatamodule>[0m
/groups/icecube/moust/storage/event_selections/event_no_numu_track_energy_15_200_nhits_4_400_sorted.csv
[[36m2023-03-07 11:51:49,156[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating model <src.models.simple_transformer_encoder_pooling_module.SimpleTransformerEncoderPoolingLitModule>[0m
[[36m2023-03-07 11:51:49,196[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpxj_4ttwl[0m
[[36m2023-03-07 11:51:49,196[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpxj_4ttwl/_remote_module_non_sriptable.py[0m
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
[[36m2023-03-07 11:51:49,219[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2023-03-07 11:51:49,219[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2023-03-07 11:51:49,221[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>[0m
[[36m2023-03-07 11:51:49,222[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2023-03-07 11:51:49,222[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2023-03-07 11:51:49,223[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2023-03-07 11:51:49,223[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: moustholmes (graphnet-team). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-07_11-51-48/wandb/run-20230307_115150-u6bcsj52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-bee-13
wandb: â­ï¸ View project at https://wandb.ai/graphnet-team/lightning-hydra-template
wandb: ğŸš€ View run at https://wandb.ai/graphnet-team/lightning-hydra-template/runs/u6bcsj52
[[36m2023-03-07 11:51:55,039[0m][[34m__main__[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
/groups/icecube/moust/miniconda3/envs/hydra/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python src/train.py trainer=gpu ...
  rank_zero_warn(
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[[36m2023-03-07 11:51:55,108[0m][[34m__main__[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2023-03-07 11:51:55,113[0m][[34m__main__[0m][[32mINFO[0m] - Starting training![0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                                                  â”ƒ Type   â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ model                                                 â”‚ Simplâ€¦ â”‚ 63.4 K â”‚
â”‚ 1  â”‚ model.fc_in                                           â”‚ Linear â”‚    288 â”‚
â”‚ 2  â”‚ model.encoder_layer                                   â”‚ Transâ€¦ â”‚ 21.0 K â”‚
â”‚ 3  â”‚ model.encoder_layer.self_attn                         â”‚ Multiâ€¦ â”‚  4.2 K â”‚
â”‚ 4  â”‚ model.encoder_layer.self_attn.out_proj                â”‚ NonDyâ€¦ â”‚  1.1 K â”‚
â”‚ 5  â”‚ model.encoder_layer.linear1                           â”‚ Linear â”‚  8.4 K â”‚
â”‚ 6  â”‚ model.encoder_layer.dropout                           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 7  â”‚ model.encoder_layer.linear2                           â”‚ Linear â”‚  8.2 K â”‚
â”‚ 8  â”‚ model.encoder_layer.norm1                             â”‚ Layerâ€¦ â”‚     64 â”‚
â”‚ 9  â”‚ model.encoder_layer.norm2                             â”‚ Layerâ€¦ â”‚     64 â”‚
â”‚ 10 â”‚ model.encoder_layer.dropout1                          â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 11 â”‚ model.encoder_layer.dropout2                          â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 12 â”‚ model.transformer_encoder                             â”‚ Transâ€¦ â”‚ 42.0 K â”‚
â”‚ 13 â”‚ model.transformer_encoder.layers                      â”‚ Modulâ€¦ â”‚ 42.0 K â”‚
â”‚ 14 â”‚ model.transformer_encoder.layers.0                    â”‚ Transâ€¦ â”‚ 21.0 K â”‚
â”‚ 15 â”‚ model.transformer_encoder.layers.0.self_attn          â”‚ Multiâ€¦ â”‚  4.2 K â”‚
â”‚ 16 â”‚ model.transformer_encoder.layers.0.self_attn.out_proj â”‚ NonDyâ€¦ â”‚  1.1 K â”‚
â”‚ 17 â”‚ model.transformer_encoder.layers.0.linear1            â”‚ Linear â”‚  8.4 K â”‚
â”‚ 18 â”‚ model.transformer_encoder.layers.0.dropout            â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 19 â”‚ model.transformer_encoder.layers.0.linear2            â”‚ Linear â”‚  8.2 K â”‚
â”‚ 20 â”‚ model.transformer_encoder.layers.0.norm1              â”‚ Layerâ€¦ â”‚     64 â”‚
â”‚ 21 â”‚ model.transformer_encoder.layers.0.norm2              â”‚ Layerâ€¦ â”‚     64 â”‚
â”‚ 22 â”‚ model.transformer_encoder.layers.0.dropout1           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 23 â”‚ model.transformer_encoder.layers.0.dropout2           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 24 â”‚ model.transformer_encoder.layers.1                    â”‚ Transâ€¦ â”‚ 21.0 K â”‚
â”‚ 25 â”‚ model.transformer_encoder.layers.1.self_attn          â”‚ Multiâ€¦ â”‚  4.2 K â”‚
â”‚ 26 â”‚ model.transformer_encoder.layers.1.self_attn.out_proj â”‚ NonDyâ€¦ â”‚  1.1 K â”‚
â”‚ 27 â”‚ model.transformer_encoder.layers.1.linear1            â”‚ Linear â”‚  8.4 K â”‚
â”‚ 28 â”‚ model.transformer_encoder.layers.1.dropout            â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 29 â”‚ model.transformer_encoder.layers.1.linear2            â”‚ Linear â”‚  8.2 K â”‚
â”‚ 30 â”‚ model.transformer_encoder.layers.1.norm1              â”‚ Layerâ€¦ â”‚     64 â”‚
â”‚ 31 â”‚ model.transformer_encoder.layers.1.norm2              â”‚ Layerâ€¦ â”‚     64 â”‚
â”‚ 32 â”‚ model.transformer_encoder.layers.1.dropout1           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 33 â”‚ model.transformer_encoder.layers.1.dropout2           â”‚ Dropoâ€¦ â”‚      0 â”‚
â”‚ 34 â”‚ model.fc_out                                          â”‚ Linear â”‚     33 â”‚
â”‚ 35 â”‚ loss_fn                                               â”‚ MSELoâ€¦ â”‚      0 â”‚
â”‚ 36 â”‚ train_loss                                            â”‚ MeanMâ€¦ â”‚      0 â”‚
â”‚ 37 â”‚ val_loss                                              â”‚ MeanMâ€¦ â”‚      0 â”‚
â”‚ 38 â”‚ test_loss                                             â”‚ MeanMâ€¦ â”‚      0 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 63.4 K                                                        
Non-trainable params: 0                                                         
Total params: 63.4 K                                                            
Total estimated model params size (MB): 0                                       
`Trainer.fit` stopped: `max_epochs=30` reached.
Epoch 29/29 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1910/1910 0:01:34 â€¢       24.51it/s loss: 968 v_num:
                                      0:00:00                   sj52 val/loss:  
                                                                1096.283        
                                                                train/loss:     
                                                                978.182         
[[36m2023-03-07 12:34:25,260[0m][[34m__main__[0m][[32mINFO[0m] - Starting testing![0m
Restoring states from the checkpoint path at /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-07_11-51-48/checkpoints/epoch_028.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
Loaded model weights from checkpoint at /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-07_11-51-48/checkpoints/epoch_028.ckpt
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ        Test metric        â”ƒ       DataLoader 0        â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚         test/loss         â”‚     968.3566284179688     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Testing â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 211/211 0:00:06 â€¢ 0:00:00 34.23it/s 
[[36m2023-03-07 12:34:36,183[0m][[34m__main__[0m][[32mINFO[0m] - Best ckpt path: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-07_11-51-48/checkpoints/epoch_028.ckpt[0m
[[36m2023-03-07 12:34:36,184[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /lustre/hpc/icecube/moust/work/GraphNet-lightning-hydra/logs/train/runs/2023-03-07_11-51-48[0m
[[36m2023-03-07 12:34:36,184[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2023-03-07 12:34:36,184[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:           test/loss â–
wandb:          train/loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:            val/loss â–ˆâ–ˆâ–‡â–…â–…â–„â–„â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:               epoch 30
wandb:           test/loss 968.35663
wandb:          train/loss 978.18201
wandb: trainer/global_step 50940
wandb:            val/loss 1096.28296
wandb: 
wandb: ğŸš€ View run leafy-bee-13 at: https://wandb.ai/graphnet-team/lightning-hydra-template/runs/u6bcsj52
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./logs/train/runs/2023-03-07_11-51-48/wandb/run-20230307_115150-u6bcsj52/logs
[[36m2023-03-07 12:34:45,504[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Metric name is None! Skipping metric value retrieval...[0m
